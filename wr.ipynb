{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCSJfNxy2LQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.collocations import *\n",
        "try:\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "#stopwords\n",
        "import re, nltk, spacy, gensim\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mwyav1m2n-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from pydrive.drive import GoogleDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "myfile = drive.CreateFile({'id': ''})\n",
        "#df=pd.read_csv('gdrive/My Drive/ColabNotebooks/sample_data/googleplaystore_user_reviews.csv')\n",
        "myfile.GetContentFile('snli_1.0_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiBU8oqa2yDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "#frm = pd.read_csv('googleplaystore_user_reviews.csv', header=None)\n",
        "df = pd.read_csv('snli_1.0_test.csv')\n",
        "df = df.dropna(subset=['sentence1'])\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3E7vokJ20ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to list\n",
        "data = df.sentence1.values.tolist()\n",
        "# Remove \n",
        "data = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "# Remove new line characters\n",
        "data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n",
        "# Remove https \n",
        "data = [re.sub(r'http\\S+', '', sent) for sent in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3m5HbUO24OS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy8sPRmI2_7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): #'NOUN', 'ADJ', 'VERB', 'ADV'\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
        "    return texts_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiakIvn83Dr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize spacy ‘en’ model, keeping only tagger component (for efficiency)\n",
        "# Run in terminal: python -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
        "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) #select noun and verb\n",
        "print(data_lemmatized[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slCX_fug3IwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import itertools\n",
        "nltk.download('punkt')\n",
        "from nltk import FreqDist\n",
        "#from nltk import FreqDis\n",
        "from nltk import word_tokenize \n",
        "from nltk.util import ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n2SP6m_3ToC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_graph(data):\n",
        "  global  window_counter, af,model, dict_test, tau, t,asz\n",
        "  # Combine all training text into one large string\n",
        "  all_text1 = (data_lemmatized)\n",
        "  edges1 = []\n",
        "  ah = [ascii]\n",
        "  window_counter = 0\n",
        "  lstt = []\n",
        "  see = 0\n",
        "  for line in all_text1:\n",
        "    token = nltk.word_tokenize(line)\n",
        "    bigram = list(ngrams(token, 2))\n",
        "    edges1.append(bigram)\n",
        "    edges = list(itertools.chain(*edges1))\n",
        "\n",
        "  return edges"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG8_XAOi3Ou7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram_edges_graph = create_graph(data)\n",
        "#print(bigram_edges_graph)\n",
        "print(len(bigram_edges_graph))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZpmPHoh3d6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import json\n",
        "import datetime\n",
        "import asyncio\n",
        "import random\n",
        "import sys\n",
        "import csv\n",
        "import requests\n",
        "#@title Capture Twitter streaming (double-click to view) {display-mode: \"form\"}\n",
        "\n",
        "k =  200#@param {type:\"number\"}\n",
        "#tau = 3 #@param {type:\"number\"}\n",
        "#lamb = 1 #@param {type:\"number\"}\n",
        "#tracking = \"bitcoin\" #@param {type:\"string\"}\n",
        "#threshold = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "#timeout = 5 #@param {type:\"slider\", min:1, max:120, step:1}\n",
        "#imeout = int(timeout)*60000\n",
        "#start = int(round(time.time() * 1000))\n",
        "\n",
        "global L,see, sample_window_stream, global_counter, time_counter, interval_counter,m,kr\n",
        "#L=0\n",
        "#sample_window_stream = []\n",
        "#global_counter = 0\n",
        "#interval_counter = 0\n",
        "#time_counter = (time.time()*1000)+60000\n",
        "\n",
        "\n",
        "# k is the size of the reservoir (number of edges into the reservoir)\n",
        "\n",
        "\n",
        "# tau is the lenght of the window\n",
        "#tau =  tau*60000\n",
        "\n",
        "# lamb is the lenght of strate floato the window\n",
        "#lamb = lamb*60000\n",
        "\n",
        "# rate is the number of strates per window\n",
        "#rate = tau/lamb\n",
        "\n",
        "# M contain steps floato the window, for each strates we have a edges counter\n",
        "#M = [0] * int(rate)\n",
        "\n",
        "# init it's a default parameters to configure somes variables when the reservoir strart\n",
        "init = 0\n",
        "\n",
        "reservoir = []\n",
        "\n",
        "\n",
        "def step_reservoir_sampling(edge):\n",
        "    global  model, dict_test, tau, t,m,kr\n",
        "    #m = 0\n",
        "    #for token in bigram_edges_graph:\n",
        "    if m<=k:\n",
        "      reservoir.append(edge)\n",
        "    if m>k and p<=1: \n",
        "      j = random.randint(0, 1)\n",
        "      if j <= p:\n",
        "        kr = random.randint(1,k)\n",
        "        del reservoir[kr-1]\n",
        "        reservoir.append(edge)\n",
        "      if m>k  and p>1:\n",
        "        del reservoir[kr]\n",
        "        reservoir.append(edge)\n",
        "      #m +=1\n",
        "    \n",
        "    return reservoir\n",
        "    #if j < k:\n",
        "     # if len(reservoir) < k:\n",
        "      #  reservoir.append(edge)\n",
        "       # print(reservoir)\n",
        "      #else:\n",
        "       # del reservoir[j]\n",
        "        #reservoir.append(edge)\n",
        "    #return reservoir\n",
        "\n",
        "#for edge in edges:\n",
        "#  window_reservoir_sampling = step_reservoir_sampling(edge)\n",
        "\n",
        "#print(window_reservoir_sampling)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbXOmako3rlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import gensim\n",
        "from gensim.models import phrases, word2vec\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "%matplotlib inline\n",
        "#model = word2vec.Word2Vec(bigram, size=50, min_count=3, iter=20)\n",
        "model = word2vec.Word2Vec(bigram_edges_graph, min_count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlWLpXKR3tCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global  model,m\n",
        "ah = [ascii]\n",
        "window_counter = 0\n",
        "lstt = []\n",
        "see = 0\n",
        "m = 0\n",
        "for edge in bigram_edges_graph:\n",
        "    #se = (edge[0], edge[1]),model.wv.similarity(edge[0], edge[1])\n",
        "    ah.append(model.wv.similarity(edge[0], edge[1]))\n",
        "    ww = abs(model.wv.similarity(edge[0], edge[1]))\n",
        "    see += ww\n",
        "    #print(see)\n",
        "    #see = see + ww\n",
        "    #p = (ww*k/see)\n",
        "    p = (ww*k)/see\n",
        "    #print (p)\n",
        "    #lstt.append(ww)\n",
        "      #print (lstt)\n",
        "      #print(wsss)\n",
        "    window_reservoir_sampling = step_reservoir_sampling(edge)\n",
        "    m +=1\n",
        "    window_counter += 1\n",
        "    \n",
        "    #see = lstt[(window_counter - 1)]\n",
        "print(window_reservoir_sampling)\n",
        "print(len(window_reservoir_sampling))\n",
        "#print(ah)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef8iq7JG3yX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the dictionary d\n",
        "\n",
        "d={}\n",
        "n=0\n",
        "for e in window_reservoir_sampling:\n",
        "   a=e[0]\n",
        "   b=e[1]\n",
        "   if a in d.keys(): d[a].add(b)\n",
        "   else: d[a]=set([b])\n",
        "   if b in d.keys(): d[b].add(a)\n",
        "   else: d[b]=set([a])\n",
        "   \n",
        "   \n",
        "#print (\"Dict=\", d, len(d))\n",
        "n=len(d)\n",
        "m=0\n",
        "#Breadth-first search from first point, first component\n",
        "a=list(d.keys())[0]\n",
        "\n",
        "#dc keeps b:i  if i is the length of the shortest path from a to b in the first component\n",
        "dc={}\n",
        "dc[a]=0\n",
        "for b in d[a]: \n",
        "  if b in dc.keys(): pass\n",
        "  else: dc[b]=1\n",
        "\n",
        "#Initialize S ans S1, Start iterating\n",
        "S=d[a]\n",
        "comp=[]\n",
        "S1=set([a])\n",
        "S=S.union(S1)\n",
        "#print(\"S=\",S)\n",
        "while S > S1:\n",
        "    S1=S\n",
        "    for u in S:\n",
        "       S=S.union(d[u])\n",
        "       for v in d[u]:\n",
        "         if v in dc.keys(): dc[v]=min(dc[v],dc[u]+1)\n",
        "         else: dc[v]=dc[u]+1\n",
        "for u in S:\n",
        "     m=m+len(d[u])\n",
        "\n",
        "comp.append((len(S),int(m/2),list(S)))\n",
        "  \n",
        "#print(\"Component\",comp)\n",
        "\n",
        "ST=S\n",
        "  \n",
        "#print(\"ST=\",ST)\n",
        "\n",
        "#The other components: origin must be outside ST, same treatment\n",
        "i=1\n",
        "while i<len(d):\n",
        " m=0\n",
        " while  list(d.keys())[i] not in ST:\n",
        "  a=list(d.keys())[i]\n",
        "  S=d[a]\n",
        "  S1=set([a])\n",
        "  S=S.union(S1)\n",
        "  while S > S1:\n",
        "    S1=S\n",
        "    for u in S:\n",
        "       S=S.union(d[u])\n",
        "  for u in S:\n",
        "     m=m+len(d[u])\n",
        "  comp.append((len(S),int(m/2),list(S)))\n",
        "  ST=ST.union(S)  \n",
        " i+=1     \n",
        "   \n",
        "comp1=sorted(comp,reverse=True)    \n",
        "print(\"Components\",comp1)\n",
        "#print(\"Diameter of largest component  =\",diameter(0))\n",
        "#To compute the diameter of  component i, do diameter(i)\n",
        "#List of edges of each component\n",
        "#Go through comp1 then the list of nodes and with the dictionary generate the edges\n",
        "\n",
        "compedges=[]\n",
        "j0=0\n",
        "while j0<len(comp1):\n",
        "  l1 = comp1[j0][2]\n",
        "#  print(l1, len(l1))\n",
        "  j=0\n",
        "  cp=[]\n",
        "  while j <len(l1):\n",
        "    a=l1[j]\n",
        "#  print(d[l1[j]])\n",
        "    j1=0\n",
        "    while j1< len(d[l1[j]]):\n",
        "  \t  f=(a,list(d[l1[j]])[j1])\n",
        "  \t  cp.append(f)\n",
        "  \t  j1 +=1\n",
        "\n",
        "    j +=1\n",
        "  j0 +=1\n",
        "  compedges.append(cp)\n",
        "print(\"Edges of each component\",compedges)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUKMDLwU4Vvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Components\",comp1[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcZz3lci4LLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}